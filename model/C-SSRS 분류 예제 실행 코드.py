import torch
import torch.nn as nn
import numpy as np
import os
from transformers import AutoTokenizer, RobertaModel, BertModel

# =========================================================
# 1. ì„¤ì • ë° í´ë˜ìŠ¤ ì •ì˜
# =========================================================
class Config:
    """ëª¨ë¸ ì„¤ì •ì„ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜"""
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    MAX_LEN = 128
    # í•™ìŠµ ë•Œ ì‚¬ìš©í•œ ëª¨ë¸ëª…ê³¼ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
    MODEL_R_NAME = "klue/roberta-large"
    MODEL_K_NAME = "beomi/kcbert-large"
    ENSEMBLE_WEIGHTS = (0.6, 0.4)
    EPSILON = 1e-10
    SAVE_PATH = "./" # ëª¨ë¸ íŒŒì¼ì´ ìˆëŠ” ê²½ë¡œ

class SoftEnsembleModel(nn.Module):
    """
    ì €ì¥ëœ ê°€ì¤‘ì¹˜(pth íŒŒì¼)ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ ëª¨ë¸ í´ë˜ìŠ¤
    """
    def __init__(self, roberta_name, kcbert_name):
        super(SoftEnsembleModel, self).__init__()
        self.roberta = RobertaModel.from_pretrained(roberta_name)
        self.kcbert = BertModel.from_pretrained(kcbert_name)
        self.classifier_r = nn.Linear(self.roberta.config.hidden_size, 2)
        self.classifier_k = nn.Linear(self.kcbert.config.hidden_size, 2)
        self.softmax = nn.Softmax(dim=1)
        self.w_r, self.w_k = Config.ENSEMBLE_WEIGHTS

    def forward(self, input_ids_r, mask_r, input_ids_k, mask_k):
        out_r = self.roberta(input_ids=input_ids_r, attention_mask=mask_r)
        logits_r = self.classifier_r(out_r.pooler_output)
        probs_r = self.softmax(logits_r)

        out_k = self.kcbert(input_ids=input_ids_k, attention_mask=mask_k)
        logits_k = self.classifier_k(out_k.pooler_output)
        probs_k = self.softmax(logits_k)

        ensemble_probs = self.w_r * probs_r + self.w_k * probs_k
        return torch.log(ensemble_probs + Config.EPSILON)

# =========================================================
# 2. ëª¨ë¸ ë¡œë“œ ë° ì˜ˆì¸¡ í•¨ìˆ˜
# =========================================================

def load_models():
    """ì €ì¥ëœ 3ê°œì˜ ëª¨ë¸ íŒŒì¼ì„ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œí•˜ê¸°"""
    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    files = ["model_stage1.pth", "model_stage2.pth", "model_stage3.pth"]
    for f in files:
        if not os.path.exists(os.path.join(Config.SAVE_PATH, f)):
            print(f"âŒ ì˜¤ë¥˜: '{f}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ‘‰ ì™¼ìª½ íŒŒì¼ ë©”ë‰´ì— ëª¨ë¸ íŒŒì¼ 3ê°œë¥¼ ì—…ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return None

    print("ğŸ”„ ëª¨ë¸ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ê³  ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤... (ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

    # ëª¨ë¸ ë¼ˆëŒ€ ìƒì„±
    m1 = SoftEnsembleModel(Config.MODEL_R_NAME, Config.MODEL_K_NAME).to(Config.DEVICE)
    m2 = SoftEnsembleModel(Config.MODEL_R_NAME, Config.MODEL_K_NAME).to(Config.DEVICE)
    m3 = SoftEnsembleModel(Config.MODEL_R_NAME, Config.MODEL_K_NAME).to(Config.DEVICE)

    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    try:
        m1.load_state_dict(torch.load(f"{Config.SAVE_PATH}model_stage1.pth", map_location=Config.DEVICE))
        m2.load_state_dict(torch.load(f"{Config.SAVE_PATH}model_stage2.pth", map_location=Config.DEVICE))
        m3.load_state_dict(torch.load(f"{Config.SAVE_PATH}model_stage3.pth", map_location=Config.DEVICE))

        m1.eval(); m2.eval(); m3.eval() # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        return (m1, m2, m3)
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return None

def predict_sentence(text, models, tokenizers, thresholds):
    """ë‹¨ì¼ ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ ìµœì¢… ë“±ê¸‰ì„ ì˜ˆì¸¡"""
    m1, m2, m3 = models
    tokenizer_r, tokenizer_k = tokenizers
    th1, th2, th3 = thresholds

    # ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬
    inputs_r = tokenizer_r(text, return_tensors='pt', max_length=Config.MAX_LEN, padding='max_length', truncation=True)
    inputs_k = tokenizer_k(text, return_tensors='pt', max_length=Config.MAX_LEN, padding='max_length', truncation=True)

    ids_r, mask_r = inputs_r['input_ids'].to(Config.DEVICE), inputs_r['attention_mask'].to(Config.DEVICE)
    ids_k, mask_k = inputs_k['input_ids'].to(Config.DEVICE), inputs_k['attention_mask'].to(Config.DEVICE)

    logs = {}

    with torch.no_grad():
        # Stage 1 (0 vs 1,2,3)
        prob_s1 = torch.exp(m1(ids_r, mask_r, ids_k, mask_k))[0][1].item()
        logs['S1_Risk'] = f"{prob_s1:.1%}"
        if prob_s1 < th1: return 0, logs

        # Stage 2 (1 vs 2,3)
        prob_s2 = torch.exp(m2(ids_r, mask_r, ids_k, mask_k))[0][1].item()
        logs['S2_HighRisk'] = f"{prob_s2:.1%}"
        if prob_s2 < th2: return 1, logs

        # Stage 3 (2 vs 3)
        prob_s3 = torch.exp(m3(ids_r, mask_r, ids_k, mask_k))[0][1].item()
        logs['S3_Severe'] = f"{prob_s3:.1%}"
        if prob_s3 < th3: return 2, logs
        else: return 3, logs

# =========================================================
# 3. ë©”ì¸ ì‹¤í–‰ íŒŒíŠ¸
# =========================================================
import sys

# 1. ë“±ê¸‰ë³„ ì„¤ëª…
LEVEL_DESC = {
    0: "ğŸŸ¢ ì •ìƒ (Level 0) - ìœ„í—˜ ì§•í›„ê°€ ë‚®ìŠµë‹ˆë‹¤.",
    1: "ğŸŸ¡ ê´€ì‹¬ (Level 1) - ì£½ìŒì— ëŒ€í•œ ì†Œë§ì´ë‚˜ ìì‚´ ì‚¬ê³ ê°€ ë“œëŸ¬ë‚˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤.",
    2: "ğŸŸ  ì£¼ì˜ (Level 2) - êµ¬ì²´ì ì¸ ìì‚´ ì‚¬ê³ ê°€ ì˜ì‹¬ë©ë‹ˆë‹¤.",
    3: "ğŸ”´ ì‹¬ê° (Level 3) - ìì‚´ ê³„íšì´ë‚˜ ì‹œë„ê°€ ìš°ë ¤ë˜ëŠ” ê³ ìœ„í—˜ ìƒíƒœì…ë‹ˆë‹¤."
}

# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ í™•ì¸
# ë³€ìˆ˜ê°€ ì—†ê±°ë‚˜ Noneì¸ ê²½ìš° ìƒˆë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
if 'models' not in locals() or models is None:
    models = load_models()
    if models is None:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. íŒŒì¼ ì—…ë¡œë“œ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        # sys.exit() ëŒ€ì‹  ë£¨í”„ ì§„ì… ë°©ì§€
    else:
        # í† í¬ë‚˜ì´ì € ë³€ìˆ˜ëª…ì„ ì „ì—­ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ í™•ì‹¤íˆ ì„ ì–¸
        tokenizer_r = AutoTokenizer.from_pretrained(Config.MODEL_R_NAME)
        tokenizer_k = AutoTokenizer.from_pretrained(Config.MODEL_K_NAME)
        print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ.")

# 3. ì„ê³„ê°’ ì„¤ì •
THRESHOLDS = (0.4, 0.7, 0.45)

# ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œëœ ê²½ìš°ì—ë§Œ ë£¨í”„ ì‹œì‘
if 'models' in locals() and models is not None:
    print("\n" + "="*60)
    print("ğŸ¤– ì‹¬ë¦¬ ìƒíƒœ ë¶„ì„ê¸° (AI Model)ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ğŸ’¬ ë¶„ì„í•˜ê³  ì‹¶ì€ ë¬¸ì¥ì„ ì…ë ¥í•˜ê³  Enterë¥¼ ëˆ„ë¥´ì„¸ìš”.")
    print("âŒ ì¢…ë£Œí•˜ë ¤ë©´ 'q' ë˜ëŠ” 'quit'ì„ ì…ë ¥í•˜ì„¸ìš”.")
    print("="*60)

    while True:
        try:
            user_input = input("\nğŸ“ ì…ë ¥: ")

            if user_input.lower() in ['q', 'quit', 'exit', 'ì¢…ë£Œ']:
                print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            if not user_input.strip():
                print("âš ï¸ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue

            # ë¡œë“œëœ tokenizer_r, tokenizer_k ë³€ìˆ˜ë¥¼ ì§ì ‘ ì „ë‹¬
            pred_level, logs = predict_sentence(user_input, models, (tokenizer_r, tokenizer_k), THRESHOLDS)

            print(f"\nğŸ‘‰ ë¶„ì„ ê²°ê³¼: {LEVEL_DESC[pred_level]}")
            print(f"ğŸ“Š ìƒì„¸ í™•ë¥ :")
            print(f"   - 1ë‹¨ê³„(ìœ„í—˜êµ° ì§„ì… í™•ë¥ ): {logs.get('S1_Risk', '0%')}")

            if 'S2_HighRisk' in logs:
                print(f"   - 2ë‹¨ê³„(ê³ ìœ„í—˜êµ° ì§„ì… í™•ë¥ ): {logs['S2_HighRisk']}")
            if 'S3_Severe' in logs:
                print(f"   - 3ë‹¨ê³„(ì‹¬ê°ë‹¨ê³„ ì§„ì… í™•ë¥ ): {logs['S3_Severe']}")

            print("-" * 40)

        except KeyboardInterrupt:
            print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
else:
    print("âš ï¸ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•„ í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
